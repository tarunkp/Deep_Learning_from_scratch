{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for binary calssification\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "# Derivative of sigmoid = sigmoid(a) * (1 - sigmoid(a))\n",
    "\n",
    "def tanh(A):\n",
    "    return np.tanh(F) # tanh\n",
    "# Derivative of tanh = 1 - (tanh(A))^2\n",
    "# Relation between tanh and sigmoid -  tanh(x)=2sigmoid(2x)-1\n",
    "\n",
    "# Rectified Linear Unit - Replace all negative values in vector with zero.\n",
    "# It does not activate all the neurons at the same time. \n",
    "def relu(A):\n",
    "    return np.maximum(A, 0)\n",
    "# Derivative of relu = (x>0) ? 1 : 0;\n",
    "# Derivative is 0 when x<0, so this can create dead neurons which never get activated.\n",
    "# Leaky ReLU or parameterized relu = (x>0) ? x : a*x;\n",
    "\n",
    "# Useful for calssifying three or more classes\n",
    "# Maximize one variable over others while keeping sum = 1\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "def activation_fn(F):\n",
    "    return sigmoid(F) # sigmoid\n",
    "    #return tanh(F) # tanh\n",
    "    #return relu(F) # relu\n",
    "\n",
    "    \n",
    "# Sigmoid functions and their combinations generally work better in the case of classifiers\n",
    "# Sigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem\n",
    "# ReLU function is a general activation function and is used in most cases these days\n",
    "# If we encounter a case of dead neurons in our networks the leaky ReLU function is the best choice\n",
    "# Always keep in mind that ReLU function should only be used in the hidden layers\n",
    "# As a rule of thumb, you can begin with using ReLU function and then move over to other activation functions in case ReLU doesnâ€™t provide with optimum results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
